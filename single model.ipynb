{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Time series forecasting\n",
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:41:13.688784Z",
     "iopub.execute_input": "2023-06-22T13:41:13.689130Z",
     "iopub.status.idle": "2023-06-22T13:41:13.694376Z",
     "shell.execute_reply.started": "2023-06-22T13:41:13.689105Z",
     "shell.execute_reply": "2023-06-22T13:41:13.693555Z"
    },
    "trusted": true
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Global variables and read data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "DATAPATH = \"/kaggle/input/store-sales-time-series-forecasting\"\n",
    "# DATAPATH = \"data/store-sales\"\n",
    "FIGSIZE = (14, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:05.273398Z",
     "iopub.execute_input": "2023-06-22T13:33:05.273742Z",
     "iopub.status.idle": "2023-06-22T13:33:05.277994Z",
     "shell.execute_reply.started": "2023-06-22T13:33:05.273711Z",
     "shell.execute_reply": "2023-06-22T13:33:05.277204Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(DATAPATH + '/train.csv',\n",
    "                       usecols=['id', 'store_nbr', 'family', 'date', 'sales', 'onpromotion'],\n",
    "                       dtype={\n",
    "                           'id': 'uint32',\n",
    "                           'store_nbr': 'int32',\n",
    "                           'family': 'string',\n",
    "                           'sales': 'float32',\n",
    "                           'onpromotion': 'uint32',\n",
    "                       },\n",
    "                       parse_dates=['date'])\n",
    "\n",
    "test_df = pd.read_csv(DATAPATH + '/test.csv',\n",
    "                      dtype={\n",
    "                          'id': 'uint32',\n",
    "                          'store_nbr': 'int32',\n",
    "                          'family': 'string',\n",
    "                          'onpromotion': 'uint32',\n",
    "                      },\n",
    "                      parse_dates=['date'])\n",
    "\n",
    "stores_df = pd.read_csv(DATAPATH + '/stores.csv',\n",
    "                        dtype={\n",
    "                            'store_nbr': 'int32',\n",
    "                            'city': 'string',\n",
    "                            'state': 'string',\n",
    "                            'type': 'string',\n",
    "                            'cluster': 'int32',\n",
    "                        })\n",
    "\n",
    "stores_df = stores_df.rename(columns={'type': 'store_type'})\n",
    "\n",
    "transactions_df = pd.read_csv(DATAPATH + '/transactions.csv',\n",
    "                              dtype={\n",
    "                                  'store_nbr': 'int32',\n",
    "                                  'transactions': 'uint32'\n",
    "                              },\n",
    "                              parse_dates=['date'])\n",
    "\n",
    "oil_df = pd.read_csv(DATAPATH + '/oil.csv',\n",
    "                     dtype={'dcoilwtico': 'float32'},\n",
    "                     parse_dates=['date'])\n",
    "\n",
    "holidays_df = pd.read_csv(DATAPATH + '/holidays_events.csv',\n",
    "                          dtype={\n",
    "                              'type': 'string',\n",
    "                              'locale': 'string',\n",
    "                              'locale_name': 'string',\n",
    "                              'description': 'string',\n",
    "                              'transferred': 'bool',\n",
    "                          },\n",
    "                          parse_dates=['date'])\n",
    "holidays_df = holidays_df.rename(columns={'type': 'holiday_type'})\n"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:05.279187Z",
     "iopub.execute_input": "2023-06-22T13:33:05.279644Z",
     "iopub.status.idle": "2023-06-22T13:33:06.932689Z",
     "shell.execute_reply.started": "2023-06-22T13:33:05.279623Z",
     "shell.execute_reply": "2023-06-22T13:33:06.932028Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# variables permettant de créer les ensembles de modèles\n",
    "\n",
    "nb_outsamples = test_df['date'].nunique()\n",
    "nb_stores = train_df['store_nbr'].nunique()\n",
    "nb_families = train_df['family'].nunique()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:06.934389Z",
     "iopub.execute_input": "2023-06-22T13:33:06.934949Z",
     "iopub.status.idle": "2023-06-22T13:33:07.124466Z",
     "shell.execute_reply.started": "2023-06-22T13:33:06.934928Z",
     "shell.execute_reply": "2023-06-22T13:33:07.123117Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def remove_outliers(df):\n",
    "    \"\"\"\n",
    "    Retire les valeurs extrêmes causées par le tremblement de terre\n",
    "    \"\"\"\n",
    "    val = df.sales.quantile(0.99)\n",
    "    df = df.drop(df[df.sales > val].index)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = remove_outliers(train_df)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:07.125558Z",
     "iopub.execute_input": "2023-06-22T13:33:07.125845Z",
     "iopub.status.idle": "2023-06-22T13:33:07.382299Z",
     "shell.execute_reply.started": "2023-06-22T13:33:07.125820Z",
     "shell.execute_reply": "2023-06-22T13:33:07.380979Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def assign_time_ft(df):\n",
    "    \"\"\"\n",
    "    Assigne les features calendaires, temporelles, etc.\n",
    "    \"\"\"\n",
    "    dt_index = df['date'].dt\n",
    "\n",
    "    df['day'] = dt_index.day\n",
    "    df['payday'] = ((dt_index.day == 15) | dt_index.is_month_end).astype('int')\n",
    "    df['dayofweek'] = dt_index.day_of_week.astype('int')\n",
    "    df[\"dayofyear\"] = dt_index.dayofyear\n",
    "    df['weekday'] = dt_index.weekday\n",
    "    df['month'] = dt_index.month\n",
    "    df['month_end'] = dt_index.is_month_end.astype('int')\n",
    "\n",
    "    df['year'] = dt_index.year\n",
    "    df['newyear'] = df[\"dayofyear\"] == 1\n",
    "\n",
    "    df['startschool'] = dt_index.month.isin((4, 5, 8, 9))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = assign_time_ft(train_df)\n",
    "test_df = assign_time_ft(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:07.383504Z",
     "iopub.execute_input": "2023-06-22T13:33:07.383785Z",
     "iopub.status.idle": "2023-06-22T13:33:08.212325Z",
     "shell.execute_reply.started": "2023-06-22T13:33:07.383761Z",
     "shell.execute_reply": "2023-06-22T13:33:08.210994Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_oil(oil):\n",
    "    \"\"\"\n",
    "    Creation de statistiques et formations de features pour le CSV sur l'essence\n",
    "    \"\"\"\n",
    "    oil['month'] = oil['date'].dt.month\n",
    "    oil['month_avg'] = oil.groupby('month')['dcoilwtico'].transform('mean')\n",
    "\n",
    "    oil['tmp'] = oil['dcoilwtico'].map(np.isnan)\n",
    "    oil['month_avg'] = oil['tmp'] * oil['month_avg']\n",
    "    oil['month_avg'] = oil['month_avg'].astype(float)\n",
    "\n",
    "    oil['dcoilwtico'].fillna(0, inplace=True)\n",
    "    oil['dcoilwtico'] = oil['dcoilwtico'] + oil['month_avg']\n",
    "\n",
    "    oil = oil.drop(['month', 'month_avg', 'tmp'], axis=1)\n",
    "\n",
    "    return oil\n",
    "\n",
    "\n",
    "oil_df = preprocess_oil(oil_df)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:08.213388Z",
     "iopub.execute_input": "2023-06-22T13:33:08.213640Z",
     "iopub.status.idle": "2023-06-22T13:33:08.226266Z",
     "shell.execute_reply.started": "2023-06-22T13:33:08.213619Z",
     "shell.execute_reply": "2023-06-22T13:33:08.225447Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_holiday(df):\n",
    "    \"\"\"\n",
    "    Separation du CSV sur les événements en df de jours fériés/vacances, tremblement de terre et fêtes\n",
    "    \"\"\"\n",
    "    filtered_holiday = df[(df['transferred'] == False) & (df['holiday_type'] != 'Work Day')]\n",
    "\n",
    "    event = df[df['holiday_type'] == 'Event']\n",
    "    earthquake = event[event['description'].str.startswith('Terremoto Manabi')]\n",
    "    event = event[event['description'].str.startswith('Terremoto Manabi') == False]\n",
    "\n",
    "    return filtered_holiday, event, earthquake\n",
    "\n",
    "\n",
    "filtered_df, event_df, earthquake_df = preprocess_holiday(holidays_df)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:08.227378Z",
     "iopub.execute_input": "2023-06-22T13:33:08.227631Z",
     "iopub.status.idle": "2023-06-22T13:33:08.238138Z",
     "shell.execute_reply.started": "2023-06-22T13:33:08.227610Z",
     "shell.execute_reply": "2023-06-22T13:33:08.237254Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "event_df = event_df[['date', 'description']]\n",
    "event_df.rename({'description': 'event_name'}, axis=1, inplace=True)\n",
    "\n",
    "earthquake_df = earthquake_df[['date', 'description']]\n",
    "earthquake_df.rename({'description': 'earthquake'}, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:08.239449Z",
     "iopub.execute_input": "2023-06-22T13:33:08.239681Z",
     "iopub.status.idle": "2023-06-22T13:33:08.255904Z",
     "shell.execute_reply.started": "2023-06-22T13:33:08.239661Z",
     "shell.execute_reply": "2023-06-22T13:33:08.255049Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Séparation en différentes échelles : locale, régionale et nationale\n",
    "\n",
    "h_local = filtered_df[filtered_df['locale'] == 'Local']\n",
    "h_local = h_local[['date', 'locale_name', 'description']]\n",
    "h_local = h_local.rename({'locale_name': 'city', 'description': 'local_hname'}, axis=1)\n",
    "\n",
    "h_regional = filtered_df[filtered_df['locale'] == 'Regional']\n",
    "h_regional = h_regional[['date', 'locale_name', 'description']]\n",
    "h_regional = h_regional.rename({'locale_name': 'state', 'description': 'regional_hname'}, axis=1)\n",
    "\n",
    "h_national = filtered_df[filtered_df['locale'] == 'National']\n",
    "h_national = h_national[['date', 'description']]\n",
    "h_national = h_national.rename({'description': 'national_hname'}, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:08.259247Z",
     "iopub.execute_input": "2023-06-22T13:33:08.259621Z",
     "iopub.status.idle": "2023-06-22T13:33:08.274276Z",
     "shell.execute_reply.started": "2023-06-22T13:33:08.259587Z",
     "shell.execute_reply": "2023-06-22T13:33:08.273457Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def merge_tables(df):\n",
    "    \"\"\"\n",
    "    Rassemblement de toutes les informations dans un seul Dataframe\n",
    "    \"\"\"\n",
    "    df = df.merge(oil_df, on='date', how='left')\n",
    "    df = df.merge(stores_df, on='store_nbr', how='left')\n",
    "\n",
    "    df = df.merge(event_df, on='date', how='left')\n",
    "    df = df.merge(earthquake_df, on='date', how='left')\n",
    "    df = df.merge(h_local, on=['date', 'city'], how='left')\n",
    "    df = df.merge(h_regional, on=['date', 'state'], how='left')\n",
    "    df = df.merge(h_national, on='date', how='left')\n",
    "\n",
    "    df = df.merge(transactions_df, on=['date', 'store_nbr'], how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = merge_tables(train_df)\n",
    "test_df = merge_tables(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:08.275159Z",
     "iopub.execute_input": "2023-06-22T13:33:08.275426Z",
     "iopub.status.idle": "2023-06-22T13:33:22.368276Z",
     "shell.execute_reply.started": "2023-06-22T13:33:08.275399Z",
     "shell.execute_reply": "2023-06-22T13:33:22.366877Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# pour économiser de la mémoire\n",
    "del h_local, h_regional, h_national, earthquake_df, event_df, stores_df, oil_df, filtered_df, transactions_df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:22.369472Z",
     "iopub.execute_input": "2023-06-22T13:33:22.369762Z",
     "iopub.status.idle": "2023-06-22T13:33:22.374193Z",
     "shell.execute_reply.started": "2023-06-22T13:33:22.369738Z",
     "shell.execute_reply": "2023-06-22T13:33:22.373444Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def handle_na(df):\n",
    "    \"\"\"\n",
    "    Pour les features de type string\n",
    "    \"\"\"\n",
    "    obj_vals = ['event_name', 'earthquake', 'local_hname', 'regional_hname', 'national_hname']\n",
    "    df[obj_vals] = df[obj_vals].fillna('0')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = handle_na(train_df)\n",
    "test_df = handle_na(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:22.376392Z",
     "iopub.execute_input": "2023-06-22T13:33:22.376923Z",
     "iopub.status.idle": "2023-06-22T13:33:23.945088Z",
     "shell.execute_reply.started": "2023-06-22T13:33:22.376852Z",
     "shell.execute_reply": "2023-06-22T13:33:23.944149Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# encodage des deux critères de sous ensembles\n",
    "encode_cols = ['family', 'store_nbr']\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "for c in encode_cols:\n",
    "    train_df[c] = lb.fit_transform(train_df[c])\n",
    "    test_df[c] = lb.transform(test_df[c])\n",
    "\n",
    "del encode_cols, lb"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:23.946093Z",
     "iopub.execute_input": "2023-06-22T13:33:23.946336Z",
     "iopub.status.idle": "2023-06-22T13:33:24.603972Z",
     "shell.execute_reply.started": "2023-06-22T13:33:23.946299Z",
     "shell.execute_reply": "2023-06-22T13:33:24.602807Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# repère les produits non vendus par certains magasins afin de pouvoir adapter les prédictions\n",
    "\n",
    "df_zeros = train_df.groupby([\"store_nbr\", \"family\"]).sales.sum().reset_index()\n",
    "df_zeros = df_zeros[df_zeros.sales == 0]\n",
    "df_zeros = df_zeros.set_index(['store_nbr', 'family'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:24.607295Z",
     "iopub.execute_input": "2023-06-22T13:33:24.607605Z",
     "iopub.status.idle": "2023-06-22T13:33:24.726174Z",
     "shell.execute_reply.started": "2023-06-22T13:33:24.607579Z",
     "shell.execute_reply": "2023-06-22T13:33:24.725158Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# encodage du reste des features string/catégorique en one hot pour faciliter le traitement\n",
    "\n",
    "encode_cols = ['store_type', 'event_name', 'earthquake', 'city', 'cluster', 'state',\n",
    "               'local_hname', 'regional_hname', 'national_hname']\n",
    "\n",
    "train_df = pd.get_dummies(train_df, columns=encode_cols)\n",
    "test_df = pd.get_dummies(test_df, columns=encode_cols)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:24.727759Z",
     "iopub.execute_input": "2023-06-22T13:33:24.728026Z",
     "iopub.status.idle": "2023-06-22T13:33:30.819165Z",
     "shell.execute_reply.started": "2023-06-22T13:33:24.728001Z",
     "shell.execute_reply": "2023-06-22T13:33:30.818085Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def optimize_mem(df, for_int=False):\n",
    "    \"\"\"\n",
    "    Convertit les valeurs dans le format le plus petit possible\n",
    "    :param for_int sert à éviter les unsigned, non acceptés par XGBoost\n",
    "    \"\"\"\n",
    "    gl_fl = df.select_dtypes(include=['float'])\n",
    "    df[gl_fl.columns] = gl_fl.apply(pd.to_numeric, downcast=\"float\")\n",
    "\n",
    "    gl_int = df.select_dtypes(include=['int'])\n",
    "    down_target = \"integer\" if for_int else \"unsigned\"\n",
    "\n",
    "    df[gl_int.columns] = gl_int.apply(pd.to_numeric, downcast=down_target)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:30.820408Z",
     "iopub.execute_input": "2023-06-22T13:33:30.820696Z",
     "iopub.status.idle": "2023-06-22T13:33:30.827057Z",
     "shell.execute_reply.started": "2023-06-22T13:33:30.820674Z",
     "shell.execute_reply": "2023-06-22T13:33:30.826002Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# on retient les ids pour un future split\n",
    "train_ids = train_df.id\n",
    "test_ids = test_df.id\n",
    "\n",
    "# on concatene tout pour faire les operations lags, rolling, etc\n",
    "all_data = pd.concat([train_df, test_df]).reset_index()\n",
    "all_data = optimize_mem(all_data)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:33:30.828588Z",
     "iopub.execute_input": "2023-06-22T13:33:30.828940Z",
     "iopub.status.idle": "2023-06-22T13:34:10.447585Z",
     "shell.execute_reply.started": "2023-06-22T13:33:30.828914Z",
     "shell.execute_reply": "2023-06-22T13:34:10.446524Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def lag_ft(df, lag_limit):\n",
    "    \"\"\"\n",
    "    Création des lags features\n",
    "    :param lag_limit sert à déterminer le nombre de lags à créer\n",
    "    \"\"\"\n",
    "    targets = ['sales', 'dcoilwtico', 'transactions']\n",
    "    keys = ['store_nbr', 'family']  # sert à limiter les statistiques au meme magasin & produit\n",
    "\n",
    "    lag_df = pd.DataFrame()\n",
    "    for target in targets:\n",
    "        for lag in range(1, lag_limit + 1):\n",
    "            lag_df[target + '_lag_' + str(lag)] = df.groupby(keys)[target].shift(lag)\n",
    "\n",
    "    return lag_df\n",
    "\n",
    "\n",
    "lag_df = lag_ft(all_data[['store_nbr', 'family', 'sales', 'dcoilwtico', 'transactions']], lag_limit=6)\n",
    "all_data = all_data.join(lag_df)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:34:10.467849Z",
     "iopub.execute_input": "2023-06-22T13:34:10.468262Z",
     "iopub.status.idle": "2023-06-22T13:34:13.503416Z",
     "shell.execute_reply.started": "2023-06-22T13:34:10.468236Z",
     "shell.execute_reply": "2023-06-22T13:34:13.502632Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "all_data = all_data.drop_duplicates(subset=['id'], keep='first')\n",
    "all_data = optimize_mem(all_data)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T13:34:13.504302Z",
     "iopub.execute_input": "2023-06-22T13:34:13.505326Z",
     "iopub.status.idle": "2023-06-22T13:34:36.631110Z",
     "shell.execute_reply.started": "2023-06-22T13:34:13.505289Z",
     "shell.execute_reply": "2023-06-22T13:34:36.629660Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_rolling_ft(new_df):\n",
    "    \"\"\"\n",
    "    Création de rolling features, avec average, min et max.\n",
    "    \"\"\"\n",
    "    targets = ['sales', 'dcoilwtico']\n",
    "    rollings = [3, 7, 14, 30]  # sert à limiter les statistiques au meme magasin & produit\n",
    "\n",
    "    shift_df = pd.DataFrame()\n",
    "\n",
    "    for target in targets:\n",
    "        print(f'Creating {target} features')\n",
    "        grouped = new_df.groupby([\"store_nbr\", \"family\"])[target]\n",
    "\n",
    "        for rollval in rollings:\n",
    "            results = {}\n",
    "\n",
    "            avg_roll = grouped.rolling(rollval).mean()\n",
    "            max_roll = grouped.rolling(rollval).max()\n",
    "            min_roll = grouped.rolling(rollval).min()\n",
    "\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag7_avg\"] = avg_roll.shift(7).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag7_max\"] = max_roll.shift(7).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag7_min\"] = min_roll.shift(7).values\n",
    "\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag30_avg\"] = avg_roll.shift(30).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag30_max\"] = max_roll.shift(30).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag30_min\"] = min_roll.shift(30).values\n",
    "\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag60_avg\"] = avg_roll.shift(60).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag60_max\"] = max_roll.shift(60).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag60_min\"] = min_roll.shift(60).values\n",
    "\n",
    "            result_df = pd.DataFrame.from_dict(results)\n",
    "            shift_df = pd.concat([shift_df, result_df], axis=1)\n",
    "\n",
    "        shift_df = optimize_mem(shift_df)\n",
    "\n",
    "    return shift_df\n",
    "\n",
    "\n",
    "print(\"Creating rolling features\")\n",
    "sort_df = all_data[[\"store_nbr\", \"family\", \"date\", 'sales', 'dcoilwtico']].sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "res_df = create_rolling_ft(sort_df)\n",
    "all_data = all_data.join(res_df)\n",
    "print('Out')"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:34:36.632679Z",
     "iopub.execute_input": "2023-06-22T13:34:36.633083Z",
     "iopub.status.idle": "2023-06-22T13:35:10.405668Z",
     "shell.execute_reply.started": "2023-06-22T13:34:36.633047Z",
     "shell.execute_reply": "2023-06-22T13:35:10.404266Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "text": "Creating rolling features\nCreating sales features\nCreating dcoilwtico features\nOut\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def create_smallrolling_ft(new_df):\n",
    "    \"\"\"\n",
    "    Même fonction mais avec moins de paramètres afin de ne pas surcharger la mémoire\n",
    "    \"\"\"\n",
    "    targets = ['transactions']\n",
    "    rollings = [3, 5, 7]\n",
    "\n",
    "    shift_df = pd.DataFrame()\n",
    "\n",
    "    for target in targets:\n",
    "        print(f'Creating {target} features')\n",
    "        grouped = new_df.groupby([\"store_nbr\", \"family\"])[target]\n",
    "\n",
    "        for rollval in rollings:\n",
    "            results = {}\n",
    "\n",
    "            avg_roll = grouped.rolling(rollval).mean()\n",
    "            max_roll = grouped.rolling(rollval).max()\n",
    "            min_roll = grouped.rolling(rollval).min()\n",
    "\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag7_avg\"] = avg_roll.shift(7).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag7_max\"] = max_roll.shift(7).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag7_min\"] = min_roll.shift(7).values\n",
    "\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag30_avg\"] = avg_roll.shift(30).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag30_max\"] = max_roll.shift(30).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag30_min\"] = min_roll.shift(30).values\n",
    "\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag60_avg\"] = avg_roll.shift(60).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag60_max\"] = max_roll.shift(60).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag60_min\"] = min_roll.shift(60).values\n",
    "\n",
    "            result_df = pd.DataFrame.from_dict(results)\n",
    "            shift_df = pd.concat([shift_df, result_df], axis=1)\n",
    "\n",
    "        shift_df = optimize_mem(shift_df)\n",
    "\n",
    "    return shift_df\n",
    "\n",
    "\n",
    "print(\"Creating small rolling features\")\n",
    "sort_df = all_data[[\"store_nbr\", \"family\", \"date\", 'transactions']].sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "res_df = create_smallrolling_ft(sort_df)\n",
    "all_data = all_data.join(res_df)\n",
    "print('Out')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T13:35:10.407277Z",
     "iopub.execute_input": "2023-06-22T13:35:10.407664Z",
     "iopub.status.idle": "2023-06-22T13:35:20.149801Z",
     "shell.execute_reply.started": "2023-06-22T13:35:10.407636Z",
     "shell.execute_reply": "2023-06-22T13:35:20.148507Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": "Creating small rolling features\nCreating transactions features\nOut\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def create_exp_mov_av(df):\n",
    "    \"\"\"\n",
    "    Exponential moving average\n",
    "    \"\"\"\n",
    "    targets = ['sales', 'dcoilwtico', 'transactions']\n",
    "    span = [7, 16, 30, 60]\n",
    "\n",
    "    ewm_df = pd.DataFrame()\n",
    "\n",
    "    for target in targets:\n",
    "        print(f'Creating {target} features')\n",
    "        grouped = df.groupby([\"store_nbr\", \"family\"])[target]\n",
    "\n",
    "        for sp in span:\n",
    "            ewm_df[f'{target}_ewm_span_{sp}'] = grouped.ewm(span=sp).mean().values\n",
    "\n",
    "        ewm_df = optimize_mem(ewm_df)\n",
    "\n",
    "    return ewm_df\n",
    "\n",
    "\n",
    "print(\"Creating ewm features\")\n",
    "res_df = create_exp_mov_av(all_data[[\"store_nbr\", \"family\", 'sales', 'dcoilwtico', 'transactions']])\n",
    "all_data = all_data.join(res_df)\n",
    "print('Out')"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:35:20.151359Z",
     "iopub.execute_input": "2023-06-22T13:35:20.151941Z",
     "iopub.status.idle": "2023-06-22T13:35:34.455110Z",
     "shell.execute_reply.started": "2023-06-22T13:35:20.151918Z",
     "shell.execute_reply": "2023-06-22T13:35:34.454081Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": "Creating ewm features\nCreating sales features\nCreating dcoilwtico features\nCreating transactions features\nOut\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# sert à économiser de la mémoire\n",
    "del res_df, lag_df, sort_df"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:35:34.456380Z",
     "iopub.execute_input": "2023-06-22T13:35:34.457399Z",
     "iopub.status.idle": "2023-06-22T13:35:34.468635Z",
     "shell.execute_reply.started": "2023-06-22T13:35:34.457369Z",
     "shell.execute_reply": "2023-06-22T13:35:34.467618Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "all_data = all_data.sort_values(['id'])\n",
    "all_data = all_data.fillna(0)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T13:35:34.470166Z",
     "iopub.execute_input": "2023-06-22T13:35:34.470457Z",
     "iopub.status.idle": "2023-06-22T13:35:39.733275Z",
     "shell.execute_reply.started": "2023-06-22T13:35:34.470432Z",
     "shell.execute_reply": "2023-06-22T13:35:39.732093Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "all_data = optimize_mem(all_data, for_int=True)  # formattage pour XGBoost"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T13:35:39.738909Z",
     "iopub.execute_input": "2023-06-22T13:35:39.739238Z",
     "iopub.status.idle": "2023-06-22T13:36:34.982784Z",
     "shell.execute_reply.started": "2023-06-22T13:35:39.739216Z",
     "shell.execute_reply": "2023-06-22T13:36:34.981775Z"
    },
    "trusted": true
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def split_dfs(df):\n",
    "    new_train = df[df['id'].isin(train_ids)]\n",
    "    new_test = df[df['id'].isin(test_ids)]\n",
    "\n",
    "    return new_train, new_test\n",
    "\n",
    "\n",
    "train_df, test_df = split_dfs(all_data)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:36:34.984089Z",
     "iopub.execute_input": "2023-06-22T13:36:34.984378Z",
     "iopub.status.idle": "2023-06-22T13:36:38.175003Z",
     "shell.execute_reply.started": "2023-06-22T13:36:34.984354Z",
     "shell.execute_reply": "2023-06-22T13:36:38.174034Z"
    },
    "trusted": true
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "del all_data"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T13:36:38.176083Z",
     "iopub.execute_input": "2023-06-22T13:36:38.176398Z",
     "iopub.status.idle": "2023-06-22T13:36:38.187455Z",
     "shell.execute_reply.started": "2023-06-22T13:36:38.176373Z",
     "shell.execute_reply": "2023-06-22T13:36:38.186107Z"
    },
    "trusted": true
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "xgb_params = {\n",
    "    #'tree_method':'gpu_hist',  # si GPU disponible et version de xgb compatible\n",
    "    #'gpu_id':0,\n",
    "    'n_estimators': 250,\n",
    "    'importance_type': 'gain',\n",
    "    'verbosity': 1,\n",
    "    'eval_metric': 'rmse',\n",
    "    'objective': 'reg:squarederror',\n",
    "    'random_state': 42,\n",
    "    'early_stopping_rounds': 30,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.03,\n",
    "    'subsample': 0.8,\n",
    "    \"colsample_bytree\": 0.4,\n",
    "}"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T13:36:38.188646Z",
     "iopub.execute_input": "2023-06-22T13:36:38.189046Z",
     "iopub.status.idle": "2023-06-22T13:36:38.200678Z",
     "shell.execute_reply.started": "2023-06-22T13:36:38.188964Z",
     "shell.execute_reply": "2023-06-22T13:36:38.199394Z"
    },
    "trusted": true
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(train, y, transfo=False):\n",
    "    \"\"\"\n",
    "    Fonction généralisée pour entraîner un modèle, avec KBest si désiré\n",
    "    Le split train/val est déterministe pour reproduire facilement les tests\n",
    "    \"\"\"\n",
    "    pipeline = None\n",
    "\n",
    "    if transfo:\n",
    "        pipeline = SelectKBest(f_regression, k=100)\n",
    "        temp_train = pipeline.fit_transform(train, y)\n",
    "    else:\n",
    "        temp_train = train\n",
    "\n",
    "    x_tr, x_v, y_tr, y_v = train_test_split(temp_train, y, train_size=split_size, random_state=42, shuffle=False)\n",
    "\n",
    "    model = xgb.XGBRegressor(**xgb_params)\n",
    "    trained_model = model.fit(x_tr, y_tr, eval_set=[(x_v, y_v)])  # eval_set permet d'éviter l'overfitting\n",
    "\n",
    "    return trained_model, x_v, y_v, pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:36:38.204592Z",
     "iopub.execute_input": "2023-06-22T13:36:38.204895Z",
     "iopub.status.idle": "2023-06-22T13:36:38.215406Z",
     "shell.execute_reply.started": "2023-06-22T13:36:38.204870Z",
     "shell.execute_reply": "2023-06-22T13:36:38.214090Z"
    },
    "trusted": true
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def make_predictions(s_model, test_data, features, pipeline=None):\n",
    "    \"\"\"\n",
    "    Création d'un df de prédictions compatible à la soumission Kaggle\n",
    "    Si KBest a été utilisé, l'applique sur les données de test\n",
    "    \"\"\"\n",
    "    pred_df = pd.DataFrame()\n",
    "    feat_df = pd.DataFrame()\n",
    "\n",
    "    feat_df['id'] = test_data['id'].copy()\n",
    "    feat_df['store_nbr'] = test_data['store_nbr'].copy()\n",
    "    feat_df['family'] = test_data['family'].copy()\n",
    "\n",
    "    df = test_data[features]\n",
    "\n",
    "    if pipeline is not None:\n",
    "        df = pipeline.transform(df)\n",
    "\n",
    "    xgb_pred = pd.Series(s_model.predict(df))\n",
    "\n",
    "    pred_df['sales'] = xgb_pred.map(lambda x: max(x, 0))  # pour éviter de prédire des valeurs négatives\n",
    "\n",
    "    feat_df.reset_index(drop=True, inplace=True)\n",
    "    pred_df['id'] = feat_df['id']\n",
    "    pred_df['store_nbr'] = feat_df['store_nbr']\n",
    "    pred_df['family'] = feat_df['family']\n",
    "\n",
    "    return pred_df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T13:44:52.960230Z",
     "iopub.execute_input": "2023-06-22T13:44:52.960608Z",
     "iopub.status.idle": "2023-06-22T13:44:52.968430Z",
     "shell.execute_reply.started": "2023-06-22T13:44:52.960580Z",
     "shell.execute_reply": "2023-06-22T13:44:52.967184Z"
    },
    "trusted": true
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def zero_unused(df_preds):\n",
    "    \"\"\"\n",
    "    Sert à mettre les prédictions à zéro sur les produits non vendus par certains magasins\n",
    "    \"\"\"\n",
    "    df_preds.loc[df_preds.set_index(['store_nbr', 'family']).index.isin(df_zeros.index), 'sales'] = 0\n",
    "\n",
    "    return df_preds[['id', 'sales']]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T13:46:21.372775Z",
     "iopub.execute_input": "2023-06-22T13:46:21.373693Z",
     "iopub.status.idle": "2023-06-22T13:46:21.382950Z",
     "shell.execute_reply.started": "2023-06-22T13:46:21.373629Z",
     "shell.execute_reply": "2023-06-22T13:46:21.382118Z"
    },
    "trusted": true
   },
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "split_size = 0.8\n",
    "drop_cols = ['id', 'date', 'sales']\n",
    "print(\"Starting training phase\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T13:36:38.243632Z",
     "iopub.execute_input": "2023-06-22T13:36:38.243874Z",
     "iopub.status.idle": "2023-06-22T13:36:38.257294Z",
     "shell.execute_reply.started": "2023-06-22T13:36:38.243849Z",
     "shell.execute_reply": "2023-06-22T13:36:38.256266Z"
    },
    "trusted": true
   },
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "text": "Starting training phase\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def show_metrics(actual, predictions, mdict):\n",
    "    \"\"\"\n",
    "    Résumé des métriques de regression\n",
    "    Si un dict est passé en paramètres, l'alimente [sert au récap des entraînements des ensembles]\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(actual, predictions)\n",
    "    mse = mean_squared_error(actual, predictions, squared=True)\n",
    "    rmsle = mean_squared_log_error(actual, predictions)\n",
    "    r2 = r2_score(actual, predictions)\n",
    "\n",
    "    print(\"\\nRegression metrics\")\n",
    "    print('MAE: {:.2f}'.format(mae))\n",
    "    print('MSE: {:.2f}'.format(mse))\n",
    "    print('RMSLE: {:.2f}'.format(rmsle))\n",
    "    print('R2: {:.2f}'.format(r2))\n",
    "\n",
    "    if mdict is not None:\n",
    "        mdict[\"mae\"].append(mae)\n",
    "        mdict[\"mse\"].append(mse)\n",
    "        mdict[\"rmsle\"].append(rmsle)\n",
    "        mdict[\"r2\"].append(r2)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-22T13:36:38.258467Z",
     "iopub.execute_input": "2023-06-22T13:36:38.258763Z",
     "iopub.status.idle": "2023-06-22T13:36:38.269490Z",
     "shell.execute_reply.started": "2023-06-22T13:36:38.258735Z",
     "shell.execute_reply": "2023-06-22T13:36:38.268393Z"
    },
    "trusted": true
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "single_features = [c for c in list(train_df.columns) if c not in drop_cols]\n",
    "\n",
    "single_model, x_val, y_val, single_pipe = train_model(train_df[single_features], train_df['sales'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"\\nEvaluating model\")\n",
    "y_pred = single_model.predict(x_val)\n",
    "y_pred[y_pred < 0] = 0\n",
    "\n",
    "show_metrics(y_val, y_pred, None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def plot_predictions(nb_samples, actual, predictions):\n",
    "    sp_list = list(range(0, nb_samples))\n",
    "\n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(sp_list, actual, label='Expected', alpha=0.5)\n",
    "    plt.plot(sp_list, predictions, label='Predicted', alpha=0.5)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title('Expected & Predicted')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Sales')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(sp_list, abs(actual - predictions))\n",
    "    plt.title('Difference (actual - preds)')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Difference')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_predictions(len(x_val), y_val, y_pred)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_names = [x for x in train_df.columns if x not in drop_cols]\n",
    "features_val = single_model.feature_importances_\n",
    "\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "ft = pd.Series(features_val, index=feature_names)\n",
    "\n",
    "nb_elem = 25\n",
    "top_features = ft.nlargest(nb_elem, keep='all').sort_values(ascending=True)\n",
    "top_features.plot.barh()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Features not shown\\n', list(ft.index.difference(top_features.index)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print('Notebook done')\n"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
