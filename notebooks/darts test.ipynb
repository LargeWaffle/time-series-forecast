{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-21T19:24:28.689443600Z",
     "start_time": "2023-05-21T19:24:28.374608100Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__init__() takes exactly one argument (the instance to initialize)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdarts\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Scaler, StaticCovariatesTransformer, MissingValuesFiller, InvertibleMapper\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdarts\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmissing_values\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m fill_missing_values\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdarts\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m XGBModel\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdarts\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MovingAverageFilter\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdarts\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtimeseries_generation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datetime_attribute_timeseries\n",
      "File \u001B[1;32mC:\\Dev\\Anaconda\\envs\\timeseries\\lib\\site-packages\\darts\\models\\__init__.py:66\u001B[0m\n\u001B[0;32m     63\u001B[0m     LightGBMModel \u001B[38;5;241m=\u001B[39m NotImportedLightGBM()\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 66\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdarts\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mforecasting\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprophet_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Prophet\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[0;32m     68\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m     69\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe prophet module could not be imported. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     70\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo enable support for Prophet model, follow \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     71\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe instruction in the README: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     72\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://github.com/unit8co/darts/blob/master/INSTALL.md\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     73\u001B[0m     )\n",
      "File \u001B[1;32mC:\\Dev\\Anaconda\\envs\\timeseries\\lib\\site-packages\\darts\\models\\forecasting\\prophet_model.py:12\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mprophet\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdarts\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlogging\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m execute_and_suppress_output, get_logger, raise_if\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdarts\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mforecasting\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mforecasting_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     16\u001B[0m     FutureCovariatesLocalForecastingModel,\n\u001B[0;32m     17\u001B[0m )\n",
      "File \u001B[1;32mC:\\Dev\\Anaconda\\envs\\timeseries\\lib\\site-packages\\prophet\\__init__.py:7\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) 2017-present, Facebook, Inc.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# All rights reserved.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# LICENSE file in the root directory of this source tree. An additional grant\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# of patent rights can be found in the PATENTS file in the same directory.\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mprophet\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mforecaster\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Prophet\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[0;32m     10\u001B[0m about \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32mC:\\Dev\\Anaconda\\envs\\timeseries\\lib\\site-packages\\prophet\\forecaster.py:19\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NDArray\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mprophet\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmake_holidays\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_holiday_names, make_holidays_df\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mprophet\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StanBackendEnum\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mprophet\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mplot\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (plot, plot_components)\n",
      "File \u001B[1;32mC:\\Dev\\Anaconda\\envs\\timeseries\\lib\\site-packages\\prophet\\make_holidays.py:14\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mprophet\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhdays\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mhdays_part2\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mholidays\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mhdays_part1\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_holiday_names\u001B[39m(country):\n",
      "File \u001B[1;32mC:\\Dev\\Anaconda\\envs\\timeseries\\lib\\site-packages\\prophet\\hdays.py:779\u001B[0m\n\u001B[0;32m    771\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    774\u001B[0m \u001B[38;5;66;03m# ------------ Holidays in Turkey---------------------\u001B[39;00m\n\u001B[0;32m    775\u001B[0m \u001B[38;5;66;03m# This is now in Holidays, but with alias TR instead of the TU that we used.\u001B[39;00m\n\u001B[0;32m    776\u001B[0m \u001B[38;5;66;03m# Include TU as an alias for backwards compatibility.\u001B[39;00m\n\u001B[1;32m--> 779\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mTU\u001B[39;00m(Turkey):\n\u001B[0;32m    780\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    783\u001B[0m \u001B[38;5;66;03m# ------------ Holidays in Pakistan---------------------\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Dev\\Anaconda\\envs\\timeseries\\lib\\site-packages\\holidays\\registry.py:178\u001B[0m, in \u001B[0;36mEntityLoader.__init__\u001B[1;34m(self, path, *args, **kwargs)\u001B[0m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentity_name \u001B[38;5;241m=\u001B[39m entity_path[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(entity_path[\u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m--> 178\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mTypeError\u001B[0m: object.__init__() takes exactly one argument (the instance to initialize)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing import Pipeline\n",
    "from darts.dataprocessing.transformers import Scaler, StaticCovariatesTransformer, MissingValuesFiller, InvertibleMapper\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "from darts.models import XGBModel\n",
    "from darts.models import MovingAverageFilter\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.metrics import mae, mse, rmse, rmsle, r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATAPATH = \"../data/store-sales\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATAPATH + '/train.csv', parse_dates=['date'])\n",
    "stores_df = pd.read_csv(DATAPATH + '/stores.csv')\n",
    "stores_df = stores_df.rename(columns={'type': 'store_type'})\n",
    "\n",
    "oil_df = pd.read_csv(DATAPATH + '/oil.csv', parse_dates=['date'])\n",
    "transactions_df = pd.read_csv(DATAPATH + '/transactions.csv', parse_dates=['date'])\n",
    "holidays_df = pd.read_csv(DATAPATH + '/holidays_events.csv', parse_dates=['date'])\n",
    "holidays_df = holidays_df.rename(columns={'type': 'holiday_type'})\n",
    "\n",
    "test_df = pd.read_csv(DATAPATH + '/test.csv', parse_dates=['date'])\n",
    "df_test_sorted = test_df.sort_values(by=['store_nbr','family'])\n",
    "\n",
    "df_sample_submission = pd.read_csv(DATAPATH + '/sample_submission.csv')\n",
    "\n",
    "family_list = train_df['family'].unique()\n",
    "store_list = stores_df['store_nbr'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df = train_df.merge(stores_df, on='store_nbr', how='left')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ts_dict = {}\n",
    "\n",
    "for family in family_list:\n",
    "    df_family = train_df.loc[train_df['family'] == family]\n",
    "\n",
    "    family_ts = TimeSeries.from_group_dataframe(\n",
    "        df_family,\n",
    "        time_col=\"date\",\n",
    "        group_cols=[\"store_nbr\", \"family\"],\n",
    "        static_cols=[\"city\", \"state\", \"store_type\", \"cluster\"],\n",
    "        value_cols=\"sales\",\n",
    "        fill_missing_dates=True,\n",
    "        freq='D')\n",
    "\n",
    "    for ts in family_ts:\n",
    "        ts = ts.astype(np.float32)\n",
    "\n",
    "    ts_dict[family] = sorted(family_ts, key=lambda ts: int(ts.static_covariates_values()[0, 0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline_dict = {}\n",
    "transform_dict = {}\n",
    "\n",
    "for key in ts_dict:\n",
    "    train_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n",
    "    static_cov_transformer = StaticCovariatesTransformer(verbose=False, transformer_cat=OneHotEncoder(), name=\"Encoder\")\n",
    "    log_transformer = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Log-Transform\")\n",
    "    train_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n",
    "\n",
    "    train_pipeline = Pipeline([train_filler,\n",
    "                               static_cov_transformer,\n",
    "                               log_transformer,\n",
    "                               train_scaler])\n",
    "\n",
    "    training_transformed = train_pipeline.fit_transform(ts_dict[key])\n",
    "    pipeline_dict[key] = train_pipeline\n",
    "    transform_dict[key] = training_transformed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_time_period = pd.date_range(start='2013-01-01', end='2017-08-31', freq='D')\n",
    "\n",
    "year = datetime_attribute_timeseries(time_index=full_time_period, attribute=\"year\")\n",
    "month = datetime_attribute_timeseries(time_index=full_time_period, attribute=\"month\")\n",
    "day = datetime_attribute_timeseries(time_index=full_time_period, attribute=\"day\")\n",
    "dayofyear = datetime_attribute_timeseries(time_index=full_time_period, attribute=\"dayofyear\")\n",
    "weekday = datetime_attribute_timeseries(time_index=full_time_period, attribute=\"dayofweek\")\n",
    "weekofyear = datetime_attribute_timeseries(time_index=full_time_period, attribute=\"weekofyear\")\n",
    "timesteps = TimeSeries.from_times_and_values(times=full_time_period,\n",
    "                                             values=np.arange(len(full_time_period)),\n",
    "                                             columns=[\"linear_increase\"])\n",
    "\n",
    "time_cov = year.stack(month).stack(day).stack(dayofyear).stack(weekday).stack(weekofyear).stack(timesteps)\n",
    "time_cov = time_cov.astype(np.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time_cov_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n",
    "time_cov_train, time_cov_val = time_cov.split_before(pd.Timestamp('20170816'))\n",
    "time_cov_scaler.fit(time_cov_train)\n",
    "time_cov_transformed = time_cov_scaler.transform(time_cov)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "oil = TimeSeries.from_dataframe(oil_df,\n",
    "                                time_col='date',\n",
    "                                value_cols=['dcoilwtico'],\n",
    "                                freq='D')\n",
    "\n",
    "oil = oil.astype(np.float32)\n",
    "\n",
    "# Transform\n",
    "oil_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n",
    "oil_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n",
    "oil_pipeline = Pipeline([oil_filler, oil_scaler])\n",
    "oil_transformed = oil_pipeline.fit_transform(oil)\n",
    "\n",
    "# Moving Averages for Oil Price\n",
    "oil_moving_average_7 = MovingAverageFilter(window=7)\n",
    "oil_moving_average_28 = MovingAverageFilter(window=28)\n",
    "\n",
    "ma_7 = oil_moving_average_7.filter(oil_transformed).astype(np.float32)\n",
    "ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"oil_ma_7\")\n",
    "ma_28 = oil_moving_average_28.filter(oil_transformed).astype(np.float32)\n",
    "ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"oil_ma_28\")\n",
    "oil_moving_averages = ma_7.stack(ma_28)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def holiday_list(df_stores):\n",
    "    listofseries = []\n",
    "\n",
    "    for i in range(0, len(df_stores)):\n",
    "        df_holiday_dummies = pd.DataFrame(columns=['date'])\n",
    "        df_holiday_dummies[\"date\"] = holidays_df[\"date\"]\n",
    "\n",
    "        df_holiday_dummies[\"national_holiday\"] = np.where(\n",
    "            ((holidays_df[\"holiday_type\"] == \"Holiday\") & (holidays_df[\"locale\"] == \"National\")), 1, 0)\n",
    "\n",
    "        df_holiday_dummies[\"earthquake_relief\"] = np.where(holidays_df['description'].str.contains('Terremoto Manabi'),\n",
    "                                                           1, 0)\n",
    "\n",
    "        df_holiday_dummies[\"christmas\"] = np.where(holidays_df['description'].str.contains('Navidad'), 1, 0)\n",
    "\n",
    "        df_holiday_dummies[\"football_event\"] = np.where(holidays_df['description'].str.contains('futbol'), 1, 0)\n",
    "\n",
    "        df_holiday_dummies[\"national_event\"] = np.where(((holidays_df[\"holiday_type\"] == \"Event\") & (\n",
    "                holidays_df[\"locale\"] == \"National\") & (~holidays_df['description'].str.contains(\n",
    "            'Terremoto Manabi')) & (~holidays_df['description'].str.contains('futbol'))), 1, 0)\n",
    "\n",
    "        df_holiday_dummies[\"work_day\"] = np.where((holidays_df[\"type\"] == \"Work Day\"), 1, 0)\n",
    "\n",
    "        df_holiday_dummies[\"local_holiday\"] = np.where(((holidays_df[\"holiday_type\"] == \"Holiday\") & (\n",
    "                (holidays_df[\"locale_name\"] == df_stores['state'][i]) | (\n",
    "                holidays_df[\"locale_name\"] == df_stores['city'][i]))), 1, 0)\n",
    "\n",
    "        listofseries.append(df_holiday_dummies)\n",
    "\n",
    "    return listofseries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_0_and_duplicates(holiday_list):\n",
    "    listofseries = []\n",
    "\n",
    "    for i in range(0, len(holiday_list)):\n",
    "        df_holiday_per_store = list_of_holidays_per_store[i].set_index('date')\n",
    "\n",
    "        df_holiday_per_store = df_holiday_per_store.loc[~(df_holiday_per_store == 0).all(axis=1)]\n",
    "\n",
    "        df_holiday_per_store = df_holiday_per_store.groupby('date').agg(\n",
    "            {'national_holiday': 'max', 'earthquake_relief': 'max',\n",
    "             'christmas': 'max', 'football_event': 'max',\n",
    "             'national_event': 'max', 'work_day': 'max',\n",
    "             'local_holiday': 'max'}).reset_index()\n",
    "\n",
    "        listofseries.append(df_holiday_per_store)\n",
    "\n",
    "    return listofseries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def holiday_TS_list_54(holiday_list):\n",
    "    listofseries = []\n",
    "\n",
    "    for i in range(0, 54):\n",
    "        holidays_ts = TimeSeries.from_dataframe(list_of_holidays_per_store[i],\n",
    "                                                time_col='date',\n",
    "                                                fill_missing_dates=True,\n",
    "                                                fillna_value=0,\n",
    "                                                freq='D')\n",
    "\n",
    "        holidays_ts = holidays_ts.slice(pd.Timestamp('20130101'), pd.Timestamp('20170831'))\n",
    "        holidays_ts = holidays_ts.astype(np.float32)\n",
    "        listofseries.append(holidays_ts)\n",
    "\n",
    "    return listofseries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_of_holidays_per_store = holiday_list(stores_df)\n",
    "list_of_holidays_per_store = remove_0_and_duplicates(list_of_holidays_per_store)\n",
    "list_of_holidays_store = holiday_TS_list_54(list_of_holidays_per_store)\n",
    "\n",
    "holidays_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n",
    "holidays_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n",
    "\n",
    "holidays_pipeline = Pipeline([holidays_filler, holidays_scaler])\n",
    "holidays_transformed = holidays_pipeline.fit_transform(list_of_holidays_store)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_promotion = pd.concat([train_df, test_df], axis=0)\n",
    "df_promotion = df_promotion.sort_values([\"store_nbr\",\"family\",\"date\"])\n",
    "df_promotion.tail()\n",
    "\n",
    "family_promotion_dict = {}\n",
    "\n",
    "for family in family_list:\n",
    "    df_family = df_promotion.loc[df_promotion['family'] == family]\n",
    "\n",
    "    list_of_TS_promo = TimeSeries.from_group_dataframe(\n",
    "        df_family,\n",
    "        time_col=\"date\",\n",
    "        group_cols=[\"store_nbr\",\"family\"],\n",
    "        value_cols=\"onpromotion\",\n",
    "        fill_missing_dates=True,\n",
    "        freq='D')\n",
    "\n",
    "    for ts in list_of_TS_promo:\n",
    "        ts = ts.astype(np.float32)\n",
    "\n",
    "    family_promotion_dict[family] = list_of_TS_promo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "promotion_transformed_dict = {}\n",
    "\n",
    "for key in family_promotion_dict:\n",
    "    promo_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n",
    "    promo_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n",
    "\n",
    "    promo_pipeline = Pipeline([promo_filler,\n",
    "                               promo_scaler])\n",
    "\n",
    "    promotion_transformed = promo_pipeline.fit_transform(family_promotion_dict[key])\n",
    "\n",
    "    # Moving Averages for Promotion Family Dictionaries\n",
    "    promo_moving_average_7 = MovingAverageFilter(window=7)\n",
    "    promo_moving_average_28 = MovingAverageFilter(window=28)\n",
    "\n",
    "    promotion_covs = []\n",
    "\n",
    "    for ts in promotion_transformed:\n",
    "        ma_7 = promo_moving_average_7.filter(ts)\n",
    "        ma_7 = TimeSeries.from_series(ma_7.pd_series())\n",
    "        ma_7 = ma_7.astype(np.float32)\n",
    "        ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"promotion_ma_7\")\n",
    "        ma_28 = promo_moving_average_28.filter(ts)\n",
    "        ma_28 = TimeSeries.from_series(ma_28.pd_series())\n",
    "        ma_28 = ma_28.astype(np.float32)\n",
    "        ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"promotion_ma_28\")\n",
    "        promo_and_mas = ts.stack(ma_7).stack(ma_28)\n",
    "        promotion_covs.append(promo_and_mas)\n",
    "\n",
    "    promotion_transformed_dict[key] = promotion_covs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "general_covariates = time_cov_transformed.stack(oil_transformed).stack(oil_moving_averages)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "store_covariates_future = []\n",
    "\n",
    "for store in range(0,len(store_list)):\n",
    "    stacked_covariates = holidays_transformed[store].stack(general_covariates)\n",
    "    store_covariates_future.append(stacked_covariates)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "future_covariates_dict = {}\n",
    "\n",
    "for key in promotion_transformed_dict:\n",
    "\n",
    "    promotion_family = promotion_transformed_dict[key]\n",
    "    covariates_future = [promotion_family[i].stack(store_covariates_future[i]) for i in range(0,len(promotion_family))]\n",
    "\n",
    "    future_covariates_dict[key] = covariates_future"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transactions_df.sort_values([\"store_nbr\",\"date\"], inplace=True)\n",
    "\n",
    "TS_transactions_list = TimeSeries.from_group_dataframe(\n",
    "    transactions_df,\n",
    "    time_col=\"date\",\n",
    "    group_cols=[\"store_nbr\"],\n",
    "    value_cols=\"transactions\",\n",
    "    fill_missing_dates=True,\n",
    "    freq='D')\n",
    "\n",
    "transactions_list = []\n",
    "\n",
    "for ts in TS_transactions_list:\n",
    "    series = TimeSeries.from_series(ts.pd_series())\n",
    "    series = series.astype(np.float32)\n",
    "    transactions_list.append(series)\n",
    "\n",
    "transactions_list[24] = transactions_list[24].slice(start_ts=pd.Timestamp('20130102'), end_ts=pd.Timestamp('20170815'))\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "transactions_list_full = []\n",
    "\n",
    "for ts in transactions_list:\n",
    "    if ts.start_time() > pd.Timestamp('20130101'):\n",
    "        end_time = (ts.start_time() - timedelta(days=1))\n",
    "        delta = end_time - pd.Timestamp('20130101')\n",
    "        zero_series = TimeSeries.from_times_and_values(\n",
    "            times=pd.date_range(start=pd.Timestamp('20130101'),\n",
    "                                end=end_time, freq=\"D\"),\n",
    "            values=np.zeros(delta.days+1))\n",
    "        ts = zero_series.append(ts)\n",
    "        ts = ts.with_columns_renamed(col_names=ts.components, col_names_new=\"transactions\")\n",
    "        transactions_list_full.append(ts)\n",
    "\n",
    "transactions_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n",
    "transactions_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n",
    "\n",
    "transactions_pipeline = Pipeline([transactions_filler, transactions_scaler])\n",
    "transactions_transformed = transactions_pipeline.fit_transform(transactions_list_full)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LGBM_Models_Submission = {}\n",
    "\n",
    "display(\"Training...\")\n",
    "\n",
    "for family in family_list:\n",
    "\n",
    "    sales_family = transform_dict[family]\n",
    "    training_data = [ts for ts in sales_family]\n",
    "    TCN_covariates = future_covariates_dict[family]\n",
    "    train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n",
    "\n",
    "    LGBM_Model_Submission = XGBModel(lags = 63,\n",
    "                                          lags_future_covariates = (14,1),\n",
    "                                          lags_past_covariates = [-16,-17,-18,-19,-20,-21,-22],\n",
    "                                          output_chunk_length=1,\n",
    "                                          random_state=2022,\n",
    "                                          gpu_use_dp= \"false\",\n",
    "                                          )\n",
    "\n",
    "    LGBM_Model_Submission.fit(series=train_sliced,\n",
    "                              future_covariates=TCN_covariates,\n",
    "                              past_covariates=transactions_transformed)\n",
    "\n",
    "    LGBM_Models_Submission[family] = LGBM_Model_Submission"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(\"Predictions...\")\n",
    "\n",
    "LGBM_Forecasts_Families_Submission = {}\n",
    "\n",
    "for family in family_list:\n",
    "\n",
    "    sales_family = transform_dict[family]\n",
    "    training_data = [ts for ts in sales_family]\n",
    "    LGBM_covariates = future_covariates_dict[family]\n",
    "    train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n",
    "\n",
    "    forecast_LGBM = LGBM_Models_Submission[family].predict(n=16,\n",
    "                                                           series=train_sliced,\n",
    "                                                           future_covariates=LGBM_covariates,\n",
    "                                                           past_covariates=transactions_transformed)\n",
    "\n",
    "    LGBM_Forecasts_Families_Submission[family] = forecast_LGBM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LGBM_Forecasts_Families_back_Submission = {}\n",
    "\n",
    "for family in family_list:\n",
    "\n",
    "    LGBM_Forecasts_Families_back_Submission[family] = pipeline_dict[family].inverse_transform(LGBM_Forecasts_Families_Submission[family], partial=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for family in LGBM_Forecasts_Families_back_Submission:\n",
    "    for n in range(0,len(LGBM_Forecasts_Families_back_Submission[family])):\n",
    "        if (ts_dict[family][n].univariate_values()[-21:] == 0).all():\n",
    "            LGBM_Forecasts_Families_back_Submission[family][n] = LGBM_Forecasts_Families_back_Submission[family][n].map(lambda x: x * 0)\n",
    "\n",
    "listofseries = []\n",
    "\n",
    "for store in range(0,54):\n",
    "    for family in family_list:\n",
    "        oneforecast = LGBM_Forecasts_Families_back_Submission[family][store].pd_dataframe()\n",
    "        oneforecast.columns = ['fcast']\n",
    "        listofseries.append(oneforecast)\n",
    "\n",
    "df_forecasts = pd.concat(listofseries)\n",
    "df_forecasts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# No Negative Forecasts\n",
    "df_forecasts[df_forecasts < 0] = 0\n",
    "forecasts_kaggle = pd.concat([df_test_sorted, df_forecasts.set_index(df_test_sorted.index)], axis=1)\n",
    "forecasts_kaggle_sorted = forecasts_kaggle.sort_values(by=['id'])\n",
    "forecasts_kaggle_sorted = forecasts_kaggle_sorted.drop(['date','store_nbr','family'], axis=1)\n",
    "forecasts_kaggle_sorted = forecasts_kaggle_sorted.rename(columns={\"fcast\": \"sales\"})\n",
    "forecasts_kaggle_sorted = forecasts_kaggle_sorted.reset_index(drop=True)\n",
    "\n",
    "# Submission\n",
    "submission_kaggle = forecasts_kaggle_sorted\n",
    "submission_kaggle.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"print(\"MAE = {:.2f}%\".format(mae(y_val, y_pred)))\n",
    "print(\"MSE = {:.2f}%\".format(mse(y_val, y_pred)))\n",
    "print(\"RMSE = {:.2f}%\".format(rmse(y_val, y_pred)))\n",
    "print(\"RMSLE = {:.2f}%\".format(rmsle(y_val, y_pred)))\n",
    "print(\"R2 = {:.2f}%\".format(r2_score(y_val, y_pred)))\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
