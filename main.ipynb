{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess, CalendarFourier\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-04T18:41:27.448968Z",
     "iopub.execute_input": "2023-06-04T18:41:27.449359Z",
     "iopub.status.idle": "2023-06-04T18:41:28.280231Z",
     "shell.execute_reply.started": "2023-06-04T18:41:27.449329Z",
     "shell.execute_reply": "2023-06-04T18:41:28.278716Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# DATAPATH = \"/kaggle/input/store-sales-time-series-forecasting\"\n",
    "DATAPATH = \"data/store-sales\"\n",
    "FIGSIZE = (14, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:41:28.282139Z",
     "iopub.execute_input": "2023-06-04T18:41:28.282556Z",
     "iopub.status.idle": "2023-06-04T18:41:28.290352Z",
     "shell.execute_reply.started": "2023-06-04T18:41:28.282519Z",
     "shell.execute_reply": "2023-06-04T18:41:28.288191Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(DATAPATH + '/train.csv', parse_dates=['date'])\n",
    "test_df = pd.read_csv(DATAPATH + '/test.csv', parse_dates=['date'])\n",
    "\n",
    "stores_df = pd.read_csv(DATAPATH + '/stores.csv')\n",
    "stores_df = stores_df.rename(columns={'type': 'store_type'})\n",
    "\n",
    "transactions_df = pd.read_csv(DATAPATH + '/transactions.csv', parse_dates=['date'])\n",
    "oil_df = pd.read_csv(DATAPATH + '/oil.csv', parse_dates=['date'])\n",
    "\n",
    "holidays_df = pd.read_csv(DATAPATH + '/holidays_events.csv', parse_dates=['date'])"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:41:28.292271Z",
     "iopub.execute_input": "2023-06-04T18:41:28.293329Z",
     "iopub.status.idle": "2023-06-04T18:41:31.109647Z",
     "shell.execute_reply.started": "2023-06-04T18:41:28.293259Z",
     "shell.execute_reply": "2023-06-04T18:41:31.108942Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def remove_outliers(df):\n",
    "    for istore in range(len(df.store_nbr.unique())):  # handling the ouliers for each store\n",
    "        val = df[df.store_nbr == istore].sales.quantile(0.99)\n",
    "        df = df.drop(df[(df.store_nbr == istore) & (df.sales > val)].index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = remove_outliers(train_df)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:41:31.111482Z",
     "iopub.execute_input": "2023-06-04T18:41:31.111788Z",
     "iopub.status.idle": "2023-06-04T18:41:42.197767Z",
     "shell.execute_reply.started": "2023-06-04T18:41:31.111764Z",
     "shell.execute_reply": "2023-06-04T18:41:42.196360Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def assign_time_ft(df):\n",
    "    df['month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    df['payday'] = ((df['date'].dt.day == 15) | df['date'].dt.is_month_end).astype(int)\n",
    "\n",
    "    df[\"dayofyear\"] = df['date'].dt.dayofyear\n",
    "\n",
    "    df[\"weekofyear\"] = df['date'].dt.isocalendar().week\n",
    "    df['weekofyear'] = df['weekofyear'].astype(np.int32)\n",
    "\n",
    "    df['weekday'] = df['date'].dt.weekday\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "\n",
    "    df['is_weekday'] = 0\n",
    "    df.loc[df['weekday'] < 5, 'is_weekday'] = 1\n",
    "\n",
    "    df[\"season\"] = df[\"month\"] % 12 // 3\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = assign_time_ft(train_df)\n",
    "test_df = assign_time_ft(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:41:42.198981Z",
     "iopub.execute_input": "2023-06-04T18:41:42.199276Z",
     "iopub.status.idle": "2023-06-04T18:41:43.637460Z",
     "shell.execute_reply.started": "2023-06-04T18:41:42.199250Z",
     "shell.execute_reply": "2023-06-04T18:41:43.636131Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_oil(oil):\n",
    "    oil['month'] = oil['date'].dt.month\n",
    "    oil['month_avg'] = oil.groupby('month')['dcoilwtico'].transform('mean')\n",
    "\n",
    "    oil['tmp'] = oil['dcoilwtico'].map(np.isnan)\n",
    "    oil['month_avg'] = oil['tmp'] * oil['month_avg']\n",
    "    oil['month_avg'] = oil['month_avg'].astype(float)\n",
    "\n",
    "    oil['dcoilwtico'].fillna(0, inplace=True)\n",
    "    oil['dcoilwtico'] = oil['dcoilwtico'] + oil['month_avg']\n",
    "\n",
    "    oil = oil.drop(['month', 'month_avg', 'tmp'], axis=1)\n",
    "\n",
    "    return oil\n",
    "\n",
    "\n",
    "oil_df = preprocess_oil(oil_df)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:41:43.639382Z",
     "iopub.execute_input": "2023-06-04T18:41:43.639789Z",
     "iopub.status.idle": "2023-06-04T18:41:43.660411Z",
     "shell.execute_reply.started": "2023-06-04T18:41:43.639754Z",
     "shell.execute_reply": "2023-06-04T18:41:43.659194Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_holiday(df):\n",
    "    filtered_holiday = df[(df['transferred'] == False) & (df['type'] != 'Work Day')]\n",
    "\n",
    "    event = df[df['type'] == 'Event']\n",
    "    earthquake = event[event['description'].str.startswith('Terremoto Manabi')]\n",
    "    event = event[event['description'].str.startswith('Terremoto Manabi') == False]\n",
    "\n",
    "    return filtered_holiday, event, earthquake\n",
    "\n",
    "\n",
    "filtered_df, event_df, earthquake_df = preprocess_holiday(holidays_df)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:41:43.661787Z",
     "iopub.execute_input": "2023-06-04T18:41:43.662193Z",
     "iopub.status.idle": "2023-06-04T18:41:43.681944Z",
     "shell.execute_reply.started": "2023-06-04T18:41:43.662166Z",
     "shell.execute_reply": "2023-06-04T18:41:43.680745Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "event_df = event_df[['date', 'description']]\n",
    "event_df.rename({'description': 'event_name'}, axis=1, inplace=True)\n",
    "\n",
    "earthquake_df = earthquake_df[['date', 'description']]\n",
    "earthquake_df.rename({'description': 'earthquake'}, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:41:43.683218Z",
     "iopub.execute_input": "2023-06-04T18:41:43.683544Z",
     "iopub.status.idle": "2023-06-04T18:41:43.694194Z",
     "shell.execute_reply.started": "2023-06-04T18:41:43.683521Z",
     "shell.execute_reply": "2023-06-04T18:41:43.693368Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "h_local = filtered_df[filtered_df['locale'] == 'Local']\n",
    "h_local = h_local[['date', 'locale_name', 'description']]\n",
    "h_local = h_local.rename({'locale_name': 'city', 'description': 'local_hname'}, axis=1)\n",
    "\n",
    "h_regional = filtered_df[filtered_df['locale'] == 'Regional']\n",
    "h_regional = h_regional[['date', 'locale_name', 'description']]\n",
    "h_regional = h_regional.rename({'locale_name': 'state', 'description': 'regional_hname'}, axis=1)\n",
    "\n",
    "h_national = filtered_df[filtered_df['locale'] == 'National']\n",
    "h_national = h_national[['date', 'description']]\n",
    "h_national = h_national.rename({'description': 'national_hname'}, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:41:43.695367Z",
     "iopub.execute_input": "2023-06-04T18:41:43.695655Z",
     "iopub.status.idle": "2023-06-04T18:41:43.712808Z",
     "shell.execute_reply.started": "2023-06-04T18:41:43.695629Z",
     "shell.execute_reply": "2023-06-04T18:41:43.711315Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def merge_tables(df):\n",
    "    df = df.merge(oil_df, on='date', how='left')\n",
    "    df = df.merge(stores_df, on='store_nbr', how='left')\n",
    "\n",
    "    df = df.merge(event_df, on='date', how='left')\n",
    "    df = df.merge(earthquake_df, on='date', how='left')\n",
    "    df = df.merge(h_local, on=['date', 'city'], how='left')\n",
    "    df = df.merge(h_regional, on=['date', 'state'], how='left')\n",
    "    df = df.merge(h_national, on='date', how='left')\n",
    "\n",
    "    df = df.merge(transactions_df, on=['date', 'store_nbr'], how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = merge_tables(train_df)\n",
    "test_df = merge_tables(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:41:43.715905Z",
     "iopub.execute_input": "2023-06-04T18:41:43.716205Z",
     "iopub.status.idle": "2023-06-04T18:41:50.986007Z",
     "shell.execute_reply.started": "2023-06-04T18:41:43.716169Z",
     "shell.execute_reply": "2023-06-04T18:41:50.984922Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "del h_local, h_regional, h_national, earthquake_df, event_df, stores_df, oil_df, filtered_df, transactions_df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-04T18:41:50.987116Z",
     "iopub.execute_input": "2023-06-04T18:41:50.987410Z",
     "iopub.status.idle": "2023-06-04T18:41:50.992800Z",
     "shell.execute_reply.started": "2023-06-04T18:41:50.987386Z",
     "shell.execute_reply": "2023-06-04T18:41:50.991596Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def handle_na(df):\n",
    "    obj_vals = ['event_name', 'earthquake', 'local_hname', 'regional_hname', 'national_hname']\n",
    "    df[obj_vals] = df[obj_vals].fillna('0')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = handle_na(train_df)\n",
    "test_df = handle_na(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:41:50.994051Z",
     "iopub.execute_input": "2023-06-04T18:41:50.994320Z",
     "iopub.status.idle": "2023-06-04T18:41:54.456419Z",
     "shell.execute_reply.started": "2023-06-04T18:41:50.994297Z",
     "shell.execute_reply": "2023-06-04T18:41:54.454970Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "encode_cols = ['family', 'store_nbr', 'city', 'state', 'store_type', 'cluster',\n",
    "               'event_name', 'earthquake', 'local_hname', 'regional_hname', 'national_hname']\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "for c in encode_cols:\n",
    "    train_df[c] = lb.fit_transform(train_df[c])\n",
    "    test_df[c] = lb.transform(test_df[c])"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:41:54.458248Z",
     "iopub.execute_input": "2023-06-04T18:41:54.458603Z",
     "iopub.status.idle": "2023-06-04T18:41:59.758947Z",
     "shell.execute_reply.started": "2023-06-04T18:41:54.458580Z",
     "shell.execute_reply": "2023-06-04T18:41:59.757934Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "all_data = pd.concat([train_df, test_df])"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:41:59.759897Z",
     "iopub.execute_input": "2023-06-04T18:41:59.760239Z",
     "iopub.status.idle": "2023-06-04T18:41:59.884286Z",
     "shell.execute_reply.started": "2023-06-04T18:41:59.760209Z",
     "shell.execute_reply": "2023-06-04T18:41:59.882845Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def lag_ft(df):\n",
    "    new_df = df.copy()\n",
    "\n",
    "    targets = ['sales', 'dcoilwtico', 'transactions']\n",
    "    keys = ['store_nbr', 'family']\n",
    "    lag_values = [16, 21, 30, 45, 60, 90, 120, 365]\n",
    "\n",
    "    for target in targets:\n",
    "        for lag in lag_values:\n",
    "            new_df[target + '_lag_' + str(lag)] = new_df.groupby(keys)[target].shift(lag)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "all_data = lag_ft(all_data)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:41:59.885571Z",
     "iopub.execute_input": "2023-06-04T18:41:59.885929Z",
     "iopub.status.idle": "2023-06-04T18:42:03.487681Z",
     "shell.execute_reply.started": "2023-06-04T18:41:59.885898Z",
     "shell.execute_reply": "2023-06-04T18:42:03.486060Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def optimize_mem(df, for_int=False):\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    if 'date' in cols:\n",
    "        cols.remove('date')\n",
    "\n",
    "    df[cols] = df[cols].apply(pd.to_numeric, downcast=\"float\")\n",
    "\n",
    "    down_target = \"integer\" if for_int else \"unsigned\"\n",
    "    df[cols] = df[cols].apply(pd.to_numeric, downcast=down_target)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "all_data = optimize_mem(all_data)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-04T18:42:03.489048Z",
     "iopub.execute_input": "2023-06-04T18:42:03.489444Z",
     "iopub.status.idle": "2023-06-04T18:42:10.752396Z",
     "shell.execute_reply.started": "2023-06-04T18:42:03.489410Z",
     "shell.execute_reply": "2023-06-04T18:42:10.751446Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_rolling_ft(new_df):\n",
    "    targets = ['sales', 'dcoilwtico', 'transactions']\n",
    "    rollings = [20, 30, 45, 60, 90, 120, 365]\n",
    "\n",
    "    shift_df = pd.DataFrame()\n",
    "\n",
    "    for target in targets:\n",
    "        print(f'Creating {target} features')\n",
    "        grouped = new_df.groupby([\"store_nbr\", \"family\"])[target]\n",
    "\n",
    "        for rollval in rollings:\n",
    "            results = {}\n",
    "\n",
    "            avg_roll = grouped.rolling(rollval).mean()\n",
    "            max_roll = grouped.rolling(rollval).max()\n",
    "            min_roll = grouped.rolling(rollval).min()\n",
    "\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag16_avg\"] = avg_roll.shift(16).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag16_max\"] = max_roll.shift(16).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag16_min\"] = min_roll.shift(16).values\n",
    "\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag30_avg\"] = avg_roll.shift(30).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag30_max\"] = max_roll.shift(30).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag30_min\"] = min_roll.shift(30).values\n",
    "\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag60_avg\"] = avg_roll.shift(60).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag60_max\"] = max_roll.shift(60).values\n",
    "            results[f\"SMA{str(rollval)}_{target}_lag60_min\"] = min_roll.shift(60).values\n",
    "\n",
    "            result_df = pd.DataFrame.from_dict(results)\n",
    "            shift_df = pd.concat([shift_df, result_df], axis=1)\n",
    "\n",
    "        shift_df = optimize_mem(shift_df)\n",
    "\n",
    "    return shift_df\n",
    "\n",
    "\n",
    "sort_df = all_data.sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "res_df = create_rolling_ft(sort_df)\n",
    "all_data = all_data.join(res_df)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:42:10.764105Z",
     "iopub.execute_input": "2023-06-04T18:42:10.764509Z",
     "iopub.status.idle": "2023-06-04T18:44:10.760719Z",
     "shell.execute_reply.started": "2023-06-04T18:42:10.764474Z",
     "shell.execute_reply": "2023-06-04T18:44:10.759335Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "del sort_df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-04T18:44:10.761970Z",
     "iopub.execute_input": "2023-06-04T18:44:10.762284Z",
     "iopub.status.idle": "2023-06-04T18:44:10.768554Z",
     "shell.execute_reply.started": "2023-06-04T18:44:10.762258Z",
     "shell.execute_reply": "2023-06-04T18:44:10.767042Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_exp_mov_av(df):\n",
    "    targets = ['sales', 'dcoilwtico', 'transactions']\n",
    "    alphas = [0.95, 0.9, 0.8, 0.7, 0.5]\n",
    "    lags = [16, 30, 60, 90]\n",
    "\n",
    "    ewm_df = pd.DataFrame()\n",
    "\n",
    "    for target in targets:\n",
    "        grouped = df.groupby([\"store_nbr\", \"family\"])[target]\n",
    "        for alpha in alphas:\n",
    "            for lag in lags:\n",
    "                results = {\n",
    "                    f'{target}_ewm_alpha_{str(alpha).replace(\".\", \"\")}_lag_{str(lag)}': grouped.shift(lag)\n",
    "                    .ewm(alpha=alpha).mean()\n",
    "                }\n",
    "                result_df = pd.DataFrame.from_dict(results)\n",
    "                ewm_df = pd.concat([ewm_df, result_df], axis=1)\n",
    "\n",
    "    return ewm_df\n",
    "\n",
    "\n",
    "res_df = create_exp_mov_av(all_data)\n",
    "all_data = all_data.join(res_df)\n",
    "all_data = all_data.sort_values(['id'])"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:44:10.770029Z",
     "iopub.execute_input": "2023-06-04T18:44:10.770386Z",
     "iopub.status.idle": "2023-06-04T18:44:18.218682Z",
     "shell.execute_reply.started": "2023-06-04T18:44:10.770358Z",
     "shell.execute_reply": "2023-06-04T18:44:18.215150Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del res_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "all_data = all_data.fillna(0)\n",
    "all_data = optimize_mem(all_data, for_int=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-04T18:44:18.222600Z",
     "iopub.execute_input": "2023-06-04T18:44:18.223213Z",
     "iopub.status.idle": "2023-06-04T18:46:23.307509Z",
     "shell.execute_reply.started": "2023-06-04T18:44:18.223186Z",
     "shell.execute_reply": "2023-06-04T18:46:23.306203Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def split_dfs(df):\n",
    "    new_train = df.iloc[:train_df.shape[0], :]\n",
    "    new_test = df.iloc[train_df.shape[0]:, :]\n",
    "\n",
    "    return new_train, new_test\n",
    "\n",
    "\n",
    "train_df, test_df = split_dfs(all_data)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:46:23.309411Z",
     "iopub.execute_input": "2023-06-04T18:46:23.309742Z",
     "iopub.status.idle": "2023-06-04T18:46:23.317786Z",
     "shell.execute_reply.started": "2023-06-04T18:46:23.309708Z",
     "shell.execute_reply": "2023-06-04T18:46:23.316484Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "del all_data"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-04T18:36:28.882560Z",
     "iopub.execute_input": "2023-06-04T18:36:28.882921Z",
     "iopub.status.idle": "2023-06-04T18:36:28.895028Z",
     "shell.execute_reply.started": "2023-06-04T18:36:28.882889Z",
     "shell.execute_reply": "2023-06-04T18:36:28.893784Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "xgb_params = {\n",
    "    'n_estimators': 75,\n",
    "    'importance_type': 'gain',\n",
    "    'verbosity': 1,\n",
    "    'eval_metric': 'rmse',\n",
    "    'objective': 'reg:squarederror',\n",
    "    'random_state': 42,\n",
    "    'early_stopping_rounds': 25\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:49:53.064013Z",
     "iopub.execute_input": "2023-06-04T18:49:53.064539Z",
     "iopub.status.idle": "2023-06-04T18:49:53.070693Z",
     "shell.execute_reply.started": "2023-06-04T18:49:53.064507Z",
     "shell.execute_reply": "2023-06-04T18:49:53.069941Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "split_size = 0.85\n",
    "drop_cols = ['id', 'date', 'sales']  # maybe dcoilwtico"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:49:57.645902Z",
     "iopub.execute_input": "2023-06-04T18:49:57.646454Z",
     "iopub.status.idle": "2023-06-04T18:49:57.655612Z",
     "shell.execute_reply.started": "2023-06-04T18:49:57.646417Z",
     "shell.execute_reply": "2023-06-04T18:49:57.653301Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(train, y):\n",
    "    temp_train = train.copy()\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=15))\n",
    "    ])\n",
    "\n",
    "    temp_train = pipeline.fit_transform(temp_train)\n",
    "\n",
    "    x_tr, x_v, y_tr, y_v = train_test_split(temp_train, y, train_size=split_size, random_state=42, shuffle=False)\n",
    "\n",
    "    model = xgb.XGBRegressor(**xgb_params)\n",
    "    trained_model = model.fit(x_tr, y_tr, eval_set=[(x_v, y_v)])\n",
    "\n",
    "    return trained_model, x_v, y_v, None"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:49:58.802110Z",
     "iopub.execute_input": "2023-06-04T18:49:58.802729Z",
     "iopub.status.idle": "2023-06-04T18:49:58.810929Z",
     "shell.execute_reply.started": "2023-06-04T18:49:58.802696Z",
     "shell.execute_reply": "2023-06-04T18:49:58.809063Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model_gridcv(train, y):\n",
    "    temp_train = train.copy()\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=15))\n",
    "    ])\n",
    "\n",
    "    temp_train = pipeline.fit_transform(temp_train)\n",
    "\n",
    "    grid_params = {\n",
    "        'max_depth': [3, 6, 8, 10],\n",
    "        'n_estimators': [50, 75, 100],\n",
    "        'subsample': [1, 0.8, 0.75],\n",
    "        'learning_rate': [0.3, 0.03, 0.003],\n",
    "    }\n",
    "\n",
    "    scoring = {\n",
    "        \"mae\": make_scorer(mean_absolute_error),\n",
    "        \"rmsle\": make_scorer(mean_squared_log_error),\n",
    "        \"r2\": make_scorer(r2_score)\n",
    "    }\n",
    "\n",
    "    _, x_v, _, y_v = train_test_split(temp_train, y, train_size=split_size, random_state=42, shuffle=False)\n",
    "\n",
    "    model = xgb.XGBRegressor(importance_type='gain', verbosity=1, eval_metric='rmse',\n",
    "                             objective='reg:squarederror', random_state=42)\n",
    "\n",
    "    clf = GridSearchCV(model, grid_params, refit='r2', scoring=scoring, verbose=1, n_jobs=1, cv=3)\n",
    "    clf.fit(temp_train, y)\n",
    "\n",
    "    print(clf.best_score_)\n",
    "    print(clf.best_params_)\n",
    "\n",
    "    return clf.best_estimator_, x_v, y_v, pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y_train = np.log1p(train_df['sales'])\n",
    "x_train = train_df.drop(drop_cols, axis=1)\n",
    "single_features = x_train.columns\n",
    "\n",
    "single_model, x_val, y_val, single_pipe = train_model(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2023-06-04T18:50:06.272556Z",
     "iopub.execute_input": "2023-06-04T18:50:06.272977Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def show_metrics(actual, predictions, mdict):\n",
    "    mae = mean_absolute_error(actual, predictions)\n",
    "    mse = mean_squared_error(actual, predictions, squared=True)\n",
    "    rmsle = mean_squared_log_error(actual, predictions)\n",
    "    r2 = r2_score(actual, predictions)\n",
    "\n",
    "    print(\"\\nRegression metrics\")\n",
    "    print('MAE: {:.2f}'.format(mae))\n",
    "    print('MSE: {:.2f}'.format(mse))\n",
    "    print('RMSLE: {:.2f}'.format(rmsle))\n",
    "    print('R2: {:.2f}'.format(r2))\n",
    "\n",
    "    if mdict is not None:\n",
    "        mdict[\"mae\"].append(mae)\n",
    "        mdict[\"mse\"].append(mse)\n",
    "        mdict[\"rmsle\"].append(rmsle)\n",
    "        mdict[\"r2\"].append(r2)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"\\nEvaluating model\")\n",
    "y_pred = single_model.predict(x_val)\n",
    "y_pred[y_pred < 0] = 0\n",
    "y_pred = np.expm1(y_pred)\n",
    "y_pred[y_pred < 0] = 0\n",
    "\n",
    "y_actual = np.expm1(y_val)\n",
    "\n",
    "show_metrics(y_actual, y_pred, None)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def plot_predictions(nb_samples, actual, predictions):\n",
    "    sp_list = list(range(0, nb_samples))\n",
    "\n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(sp_list, actual, label='Expected', alpha=0.5)\n",
    "    plt.plot(sp_list, predictions, label='Predicted', alpha=0.5)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title('Expected & Predicted')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Sales')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(sp_list, abs(actual - predictions))\n",
    "    plt.title('Difference (actual - preds)')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Difference')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_predictions(len(x_val), y_actual, y_pred)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if not single_pipe:  # if pca is applied, can't use this\n",
    "    feature_names = [x for x in train_df.columns if x not in drop_cols]\n",
    "    features_val = single_model.feature_importances_\n",
    "\n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "    ft = pd.Series(features_val, index=feature_names)\n",
    "\n",
    "    nb_elem = 25\n",
    "    top_features = ft.nlargest(nb_elem, keep='all').sort_values(ascending=True)\n",
    "    top_features.plot.barh()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('Features not shown\\n', list(ft.index.difference(top_features.index)))"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "s_v = train_df['store_nbr'].unique()\n",
    "shop_nbr = len(s_v)\n",
    "shop_preprocess = [None] * shop_nbr\n",
    "shop_models = [None] * shop_nbr\n",
    "\n",
    "shop_drop_cols = drop_cols.copy()\n",
    "shop_drop_cols.append('store_nbr')\n",
    "\n",
    "dropped_df = train_df.drop(shop_drop_cols, axis=1)\n",
    "shop_features = dropped_df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "metrics_dict = {\"mae\": [], \"mse\": [], \"rmsle\": [], \"r2\": []}\n",
    "\n",
    "for shop in range(shop_nbr):\n",
    "    shop_df = train_df[train_df.store_nbr == shop]\n",
    "    shop_y = np.log1p(shop_df['sales'])\n",
    "\n",
    "    print(f\"Training model {shop}...\")\n",
    "    shopmodel, x_val, y_val, pipe = train_model(shop_df[shop_features], shop_y)\n",
    "\n",
    "    shop_preprocess[shop] = pipe\n",
    "\n",
    "    print(f\"\\nEvaluating sub-model {shop}\")\n",
    "    y_pred = shopmodel.predict(x_val)\n",
    "    y_pred[y_pred < 0] = 0\n",
    "    y_pred = np.expm1(y_pred)\n",
    "    y_pred[y_pred < 0] = 0\n",
    "\n",
    "    shop_models[shop] = shopmodel\n",
    "\n",
    "    y_actual = np.expm1(y_val)\n",
    "    show_metrics(y_actual, y_pred, metrics_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"\\nBy shop training summary\")\n",
    "for mkey, mval in metrics_dict.items():\n",
    "    print(\"Average {} : {:.2f}\".format(mkey.upper(), mean(mval)))"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "f_v = train_df['family'].unique()\n",
    "family_nbr = len(f_v)\n",
    "family_preprocess = [None] * family_nbr\n",
    "family_models = [None] * family_nbr\n",
    "\n",
    "fam_drop_cols = drop_cols.copy()\n",
    "fam_drop_cols.append('family')\n",
    "\n",
    "dropped_df = train_df.drop(fam_drop_cols, axis=1)\n",
    "fam_features = dropped_df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "metrics_dict = {\"mae\": [], \"mse\": [], \"rmsle\": [], \"r2\": []}\n",
    "\n",
    "for fam in range(family_nbr):\n",
    "    fam_df = train_df[train_df.family == fam]\n",
    "    family_y = np.log1p(fam_df['sales'])\n",
    "\n",
    "    print(f\"Training model {fam}...\")\n",
    "    fammodel, x_val, y_val, pipe = train_model(fam_df[fam_features], family_y)\n",
    "\n",
    "    family_preprocess[fam] = pipe\n",
    "\n",
    "    print(f\"\\nEvaluating sub-model {fam}\")\n",
    "    y_pred = fammodel.predict(x_val)\n",
    "    y_pred[y_pred < 0] = 0\n",
    "    y_pred = np.expm1(y_pred)\n",
    "    y_pred[y_pred < 0] = 0\n",
    "\n",
    "    family_models[fam] = fammodel\n",
    "\n",
    "    y_actual = np.expm1(y_val)\n",
    "    show_metrics(y_actual, y_pred, metrics_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"\\nBy family training summary\")\n",
    "for mkey, mval in metrics_dict.items():\n",
    "    print(\"Average {} : {:.2f}\".format(mkey.upper(), mean(mval)))"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Submissions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def make_predictions(s_model, test_data, features, pipeline=None):\n",
    "    pred_df = pd.DataFrame()\n",
    "    id_df = pd.DataFrame()\n",
    "\n",
    "    df = test_data[features]\n",
    "\n",
    "    if pipeline is not None:\n",
    "        df = pipeline.transform(df)\n",
    "\n",
    "    xgb_pred = pd.Series(s_model.predict(df, iteration_range=(0, s_model.best_iteration)))\n",
    "\n",
    "    pred_df['sales'] = np.expm1(xgb_pred.map(lambda x: max(x, 0)))\n",
    "    pred_df['sales'][pred_df['sales'] < 0] = 0\n",
    "\n",
    "    id_df['id'] = test_data['id'].copy()\n",
    "    id_df.reset_index(drop=True, inplace=True)\n",
    "    pred_df['id'] = id_df['id']\n",
    "\n",
    "    return pred_df"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def zero_unused_families(df_preds, test_data):\n",
    "    df_zeros = train_df.groupby([\"store_nbr\", \"family\"]).sales.sum().reset_index()\n",
    "    df_zeros = df_zeros[df_zeros.sales == 0]\n",
    "\n",
    "    temp_df = test_data.copy()\n",
    "    temp_df['sales'] = df_preds['sales']\n",
    "    zeroed_df = pd.merge(temp_df, df_zeros, left_on=['store_nbr', 'family'], right_on=['store_nbr', 'family'],\n",
    "                         how='left')\n",
    "    zeroed_df.loc[zeroed_df['sales_y'] == 0, 'sales_x'] = zeroed_df['sales_y']\n",
    "\n",
    "    zeroed_df.rename(columns={'sales_x': 'sales'}, inplace=True)\n",
    "    zeroed_df = zeroed_df.drop('sales_y', axis=1)\n",
    "    return zeroed_df[['id', 'sales']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "single_preds = make_predictions(single_model, test_df, single_features, single_pipe)\n",
    "single_preds = zero_unused_families(single_preds, test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "merged_df = pd.DataFrame()\n",
    "\n",
    "for i in range(shop_nbr):\n",
    "    shop_data = test_df[test_df.store_nbr == i]\n",
    "    preds_df = make_predictions(shop_models[i], shop_data, shop_features, shop_preprocess[i])\n",
    "    preds_df = zero_unused_families(preds_df, shop_data)\n",
    "    merged_df = pd.concat([merged_df, preds_df])\n",
    "\n",
    "store_preds = merged_df.sort_values('id', ascending=True)\n",
    "store_preds = store_preds.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "merged_df = pd.DataFrame()\n",
    "\n",
    "for i in range(family_nbr):\n",
    "    fam_data = test_df[test_df.family == i]\n",
    "    preds_df = make_predictions(family_models[i], fam_data, fam_features, family_preprocess[i])\n",
    "    preds_df = zero_unused_families(preds_df, fam_data)\n",
    "    merged_df = pd.concat([merged_df, preds_df])\n",
    "\n",
    "family_preds = merged_df.sort_values('id', ascending=True)\n",
    "family_preds = family_preds.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "submission = store_preds  # family_preds or store_preds or single_preds\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('Submission saved')\n"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
